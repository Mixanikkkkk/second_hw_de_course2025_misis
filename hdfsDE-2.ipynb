{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d13bdd8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**1.Перенос файлов на сервер**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp /Users/mixanikkk/Downloads/Посещения.csv ubuntu@158.160.178.219:/tmp\n",
    "!scp /Users/mixanikkk/Downloads/Bible.txt ubuntu@158.160.178.219:/tmp\n",
    "!hadoop fs -put /tmp/Bible.txt /user/\n",
    "!hadoop fs -put /tmp/Посещения.csv /user/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "32d3b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop    4047392 2025-04-11 16:05 /user/bible.txt\r\n",
      "drwxr-xr-x   - hive   hadoop          0 2025-04-10 14:45 /user/hive\r\n",
      "-rw-r--r--   1 ubuntu hadoop        258 2025-04-15 13:12 /user/mp.py\r\n",
      "-rw-r--r--   1 ubuntu hadoop        491 2025-04-15 13:12 /user/reduce.py\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 13:37 /user/result_bible\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-10 16:03 /user/root\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-10 16:02 /user/ubuntu\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 14:32 /user/word_count_task\r\n",
      "-rw-r--r--   1 ubuntu hadoop   36443383 2025-04-11 16:31 /user/Посещения.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6f06941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning God created the heaven and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. \r\n",
      "And God said, Let there be light: and there was light. \r\n",
      "And God saw the light, that it was good: and God divided the light from the darkness. \r\n",
      "And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day. \r\n",
      "And God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters. \r\n",
      "And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so. \r\n",
      "And God called the firmament Heaven. And the evening and the morning were the second day. \r\n",
      "And God said, Let the waters under the heaven be gathered together unto one place, and let the dry land appear: and it was so. \r\n",
      "And God called the dry land Earth; and the gathering together of the waters called he Seas: and God saw that it was good. \r\n",
      "And God said, Let the earth bring forth grass, the herb yielding seed, and the fruit tree yielding fruit after his kind, whose seed is in itself, upon the earth: and it was so. \r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /user/bible.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c76bf",
   "metadata": {},
   "source": [
    "**2.1 Подсчет слов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mp.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r\"[\\S]{5,}\")\n",
    "def mapper():\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip().lower()\n",
    "        for match in pattern.finditer(line):\n",
    "            print(f\"{match.group()}\\t1\")\n",
    "if __name__ == \"__main__\":\n",
    "    mapper() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d2842b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce.py\n",
    "import sys\n",
    "\n",
    "def reducer():\n",
    "    word=next(sys.stdin).split('\\t')[0]\n",
    "    number =int(next(sys.stdin).split('\\t')[1])\n",
    "    for line in sys.stdin:\n",
    "        cur_word, cur_number = line.split('\\t')\n",
    "        cur_number = int(cur_number)\n",
    "        if cur_word!=word:\n",
    "            print(f'{word}\\t{number}')\n",
    "            word = cur_word\n",
    "            number = cur_number\n",
    "        else:\n",
    "            number+=cur_number               \n",
    "    print(f'{word}\\t{number}')     \n",
    "if __name__ == \"__main__\":\n",
    "    reducer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1be29d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.3 ms, sys: 1.62 ms, total: 72.9 ms\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! hadoop fs -cat /user/bible.txt | python3 mp.py | sort -k1| python3 reduce.py > final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b1c27c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall\t9733\r\n",
      "which\t4244\r\n",
      "their\t3859\r\n",
      "there\t2008\r\n",
      "before\t1722\r\n",
      "lord,\t1613\r\n",
      "against\t1596\r\n",
      "shalt\t1589\r\n",
      "children\t1560\r\n",
      "said,\t1556\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "cat final.txt| sort -k2,2 -n -r|head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65a8eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\r\n"
     ]
    }
   ],
   "source": [
    "!sudo find /usr/ -name hadoop-streaming.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "783c69ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxrwx 2 root root 4096 Apr 15 12:54 /usr/lib/hadoop/logs\r\n"
     ]
    }
   ],
   "source": [
    "!sudo mkdir -p /usr/lib/hadoop/logs\n",
    "!ls -ld /usr/lib/hadoop/logs\n",
    "!sudo chmod 0777  /usr/lib/hadoop/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "49842140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs balancer\r\n",
      "\t[-policy <policy>]\tthe balancing policy: datanode or blockpool\r\n",
      "\t[-threshold <threshold>]\tPercentage of disk capacity\r\n",
      "\t[-exclude [-f <hosts-file> | <comma-separated list of hosts>]]\tExcludes the specified datanodes.\r\n",
      "\t[-include [-f <hosts-file> | <comma-separated list of hosts>]]\tIncludes only the specified datanodes.\r\n",
      "\t[-source [-f <hosts-file> | <comma-separated list of hosts>]]\tPick only the specified datanodes as source nodes.\r\n",
      "\t[-blockpools <comma-separated list of blockpool ids>]\tThe balancer will only run on blockpools included in this list.\r\n",
      "\t[-idleiterations <idleiterations>]\tNumber of consecutive idle iterations (-1 for Infinite) before exit.\r\n",
      "\t[-runDuringUpgrade]\tWhether to run the balancer during an ongoing HDFS upgrade.This is usually not desired since it will not affect used space on over-utilized machines.\r\n",
      "\r\n",
      "Apr 15, 2025 12:56:56 PM Balancing took 216.0 milliseconds\r\n",
      "2025-04-15 12:56:56,234 ERROR balancer.Balancer: Exiting balancer due an exception\r\n",
      "java.lang.IllegalArgumentException: args = [-treshhold, 5]\r\n",
      "\tat org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.parse(Balancer.java:882)\r\n",
      "\tat org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:796)\r\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n",
      "\tat org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:939)\r\n"
     ]
    }
   ],
   "source": [
    "!sudo -u hdfs hdfs balancer -treshhold 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "efd76a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/mp.py': File exists\n",
      "put: `/user/reduce.py': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put mp.py /user/\n",
    "!hdfs dfs -put reduce.py /user/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "74c8ce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 ubuntu hadoop        258 2025-04-15 13:12 /user/mp.py\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/mp.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96316c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/word_count_task\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob7744826009175359823.jar tmpDir=null\n",
      "2025-04-15 14:31:58,194 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net/10.130.0.9:8032\n",
      "2025-04-15 14:31:58,417 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net/10.130.0.9:10200\n",
      "2025-04-15 14:31:58,461 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net/10.130.0.9:8032\n",
      "2025-04-15 14:31:58,461 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net/10.130.0.9:10200\n",
      "2025-04-15 14:31:58,675 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744717336963_0015\n",
      "2025-04-15 14:31:59,027 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-15 14:31:59,135 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-15 14:31:59,310 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744717336963_0015\n",
      "2025-04-15 14:31:59,312 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-15 14:31:59,518 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-15 14:31:59,519 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-15 14:31:59,611 INFO impl.YarnClientImpl: Submitted application application_1744717336963_0015\n",
      "2025-04-15 14:31:59,654 INFO mapreduce.Job: The url to track the job: http://rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net:8088/proxy/application_1744717336963_0015/\n",
      "2025-04-15 14:31:59,655 INFO mapreduce.Job: Running job: job_1744717336963_0015\n",
      "2025-04-15 14:32:06,748 INFO mapreduce.Job: Job job_1744717336963_0015 running in uber mode : false\n",
      "2025-04-15 14:32:06,750 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-15 14:32:11,818 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2025-04-15 14:32:13,831 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2025-04-15 14:32:14,837 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2025-04-15 14:32:16,848 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-15 14:32:20,872 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-04-15 14:32:25,899 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2025-04-15 14:32:26,904 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2025-04-15 14:32:30,924 INFO mapreduce.Job:  map 57% reduce 6%\n",
      "2025-04-15 14:32:32,934 INFO mapreduce.Job:  map 70% reduce 6%\n",
      "2025-04-15 14:32:36,953 INFO mapreduce.Job:  map 70% reduce 8%\n",
      "2025-04-15 14:32:37,958 INFO mapreduce.Job:  map 70% reduce 16%\n",
      "2025-04-15 14:32:38,963 INFO mapreduce.Job:  map 83% reduce 16%\n",
      "2025-04-15 14:32:42,982 INFO mapreduce.Job:  map 83% reduce 26%\n",
      "2025-04-15 14:32:43,987 INFO mapreduce.Job:  map 83% reduce 28%\n",
      "2025-04-15 14:32:44,992 INFO mapreduce.Job:  map 97% reduce 28%\n",
      "2025-04-15 14:32:48,006 INFO mapreduce.Job:  map 100% reduce 28%\n",
      "2025-04-15 14:32:49,011 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-15 14:32:49,020 INFO mapreduce.Job: Job job_1744717336963_0015 completed successfully\n",
      "2025-04-15 14:32:49,113 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=144650\n",
      "\t\tFILE: Number of bytes written=8631088\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7736881\n",
      "\t\tHDFS: Number of bytes written=276418\n",
      "\t\tHDFS: Number of read operations=105\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=25\n",
      "\t\tRack-local map tasks=5\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=430371\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=234552\n",
      "\t\tTotal time spent by all map tasks (ms)=143457\n",
      "\t\tTotal time spent by all reduce tasks (ms)=78184\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=143457\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=78184\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=440699904\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=240181248\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30383\n",
      "\t\tMap output records=266384\n",
      "\t\tMap output bytes=2548937\n",
      "\t\tMap output materialized bytes=478221\n",
      "\t\tInput split bytes=3780\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=25424\n",
      "\t\tReduce shuffle bytes=478221\n",
      "\t\tReduce input records=266384\n",
      "\t\tReduce output records=25421\n",
      "\t\tSpilled Records=532768\n",
      "\t\tShuffled Maps =90\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=90\n",
      "\t\tGC time elapsed (ms)=4824\n",
      "\t\tCPU time spent (ms)=33890\n",
      "\t\tPhysical memory (bytes) snapshot=11013730304\n",
      "\t\tVirtual memory (bytes) snapshot=143162109952\n",
      "\t\tTotal committed heap usage (bytes)=10319036416\n",
      "\t\tPeak Map Physical memory (bytes)=357376000\n",
      "\t\tPeak Map Virtual memory (bytes)=4345118720\n",
      "\t\tPeak Reduce Physical memory (bytes)=250224640\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4353937408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7733101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=276418\n",
      "2025-04-15 14:32:49,114 INFO streaming.StreamJob: Output directory: /user/word_count_task\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/word_count_task\n",
    "!yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name='wordcounter' \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files mp.py,reduce.py \\\n",
    "-mapper \"python3 mp.py map\"\\\n",
    "-reducer \"python3 reduce.py reducer\"\\\n",
    "-input /user/bible.txt \\\n",
    "-output /user/word_count_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7259e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2025-04-15 14:02 /user/word_count_task/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop      39821 2025-04-15 14:02 /user/word_count_task/part-00000\r\n",
      "-rw-r--r--   1 ubuntu hadoop      40414 2025-04-15 14:02 /user/word_count_task/part-00001\r\n",
      "-rw-r--r--   1 ubuntu hadoop      39435 2025-04-15 14:02 /user/word_count_task/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/word_count_task/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1035eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lord.\t674\r\n",
      "pray.\t5\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/word_count_task/* | grep  -E 'lord\\.|god\\.|pray\\.' | sort -t$'\\t' -k2.2nr  | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1857f1",
   "metadata": {},
   "source": [
    "**Топ 5 сайтов по посещаемости**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "85dfeed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gonzales-bautista.com/;2024-05-28 05:58:33.853479\r",
      "\r\n",
      "https://gonzales-bautista.com/;2024-05-26 03:44:36.853479\r",
      "\r\n",
      "https://www.davis-berg.com/;2024-05-31 17:22:01.853479\r",
      "\r\n",
      "https://www.montoya.com/;2024-05-29 13:41:35.853479\r",
      "\r\n",
      "https://gonzales-bautista.com/;2024-06-01 06:49:16.853479\r",
      "\r\n",
      "https://www.davis-berg.com/;2024-05-28 22:44:29.853479\r",
      "\r\n",
      "https://gonzales-bautista.com/;2024-05-27 23:48:30.853479\r",
      "\r\n",
      "https://gonzales-bautista.com/;2024-05-31 13:04:24.853479\r",
      "\r\n",
      "https://www.davis-berg.com/;2024-05-27 12:23:23.853479\r",
      "\r\n",
      "https://gonzales-bautista.com/;2024-05-27 16:52:14.853479\r",
      "\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /user/Посещения.csv | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e43dc399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mp_for_website.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mp_for_website.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def mapper():\n",
    "    for row in csv.reader(sys.stdin):\n",
    "        row= row[0]\n",
    "        url = re.split(r\"[; ]\", row, maxsplit=0)\n",
    "        print(f'{url[1]}+{url[0]}\\t1')\n",
    "if __name__ == \"__main__\":\n",
    "    mapper() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6462b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28+https://gonzales-bautista.com/\t1\r\n",
      "2024-05-26+https://gonzales-bautista.com/\t1\r\n",
      "2024-05-31+https://www.davis-berg.com/\t1\r\n",
      "2024-05-29+https://www.montoya.com/\t1\r\n",
      "2024-06-01+https://gonzales-bautista.com/\t1\r\n",
      "2024-05-28+https://www.davis-berg.com/\t1\r\n",
      "2024-05-27+https://gonzales-bautista.com/\t1\r\n",
      "2024-05-31+https://gonzales-bautista.com/\t1\r\n",
      "2024-05-27+https://www.davis-berg.com/\t1\r\n",
      "2024-05-27+https://gonzales-bautista.com/\t1\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"mp_for_website.py\", line 12, in <module>\r\n",
      "    mapper() \r\n",
      "  File \"mp_for_website.py\", line 10, in mapper\r\n",
      "    print(f'{url[1]}+{url[0]}\\t1')\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/Посещения.csv | python3 mp_for_website.py | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16de1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce_for_website.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce_for_website.py\n",
    "import sys\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    data_by_date = []\n",
    "    website=next(sys.stdin).split('\\t')[0]\n",
    "    date =website.split('+')[0]\n",
    "    number =int(next(sys.stdin).split('\\t')[1])\n",
    "    for line in sys.stdin:\n",
    "        cur_website, cur_number = line.split('\\t', 1)\n",
    "        cur_date=cur_website.split(\"+\")[0]\n",
    "        cur_number = int(cur_number)\n",
    "        if cur_website!=website:\n",
    "            if cur_date==date:\n",
    "                data_by_date.append((website, number))\n",
    "            else:\n",
    "                data_sorted=sorted(data_by_date, key=lambda x: x[1], reverse=True)\n",
    "                for site, visits in data_sorted[:5]:\n",
    "                    print(site.replace(\"+\", ' ', 1), visits)\n",
    "                data_by_date = []\n",
    "                date = cur_date\n",
    "            website = cur_website\n",
    "            number = cur_number\n",
    "        else:\n",
    "            number+=cur_number\n",
    "    data_by_date.append((website, number))\n",
    "    data_sorted=sorted(data_by_date, key=lambda x: x[1], reverse=True)\n",
    "    for site, visits in data_sorted[:5]:\n",
    "        print(site.replace(\"+\", ' ', 1), visits) \n",
    "if __name__ == \"__main__\":\n",
    "    reducer() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3bb9cf",
   "metadata": {},
   "source": [
    "Используем проверку на дату, если дата одиноковая но сайты различаются вносим сайт в список, как только дата меняется выбираем топ 5 сайтов за день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abbc35c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/websites\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob1203960904276496227.jar tmpDir=null\n",
      "2025-04-16 22:05:00,150 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net/10.130.0.9:8032\n",
      "2025-04-16 22:05:00,386 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net/10.130.0.9:10200\n",
      "2025-04-16 22:05:00,435 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net/10.130.0.9:8032\n",
      "2025-04-16 22:05:00,436 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net/10.130.0.9:10200\n",
      "2025-04-16 22:05:00,681 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744823371038_0043\n",
      "2025-04-16 22:05:01,074 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-16 22:05:01,282 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-16 22:05:01,339 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "2025-04-16 22:05:01,489 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744823371038_0043\n",
      "2025-04-16 22:05:01,491 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-16 22:05:01,676 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-16 22:05:01,676 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-16 22:05:01,749 INFO impl.YarnClientImpl: Submitted application application_1744823371038_0043\n",
      "2025-04-16 22:05:01,785 INFO mapreduce.Job: The url to track the job: http://rc1d-dataproc-m-j7ha63uqu54o1bvr.mdb.yandexcloud.net:8088/proxy/application_1744823371038_0043/\n",
      "2025-04-16 22:05:01,786 INFO mapreduce.Job: Running job: job_1744823371038_0043\n",
      "2025-04-16 22:05:08,874 INFO mapreduce.Job: Job job_1744823371038_0043 running in uber mode : false\n",
      "2025-04-16 22:05:08,876 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-16 22:05:13,932 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2025-04-16 22:05:15,948 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2025-04-16 22:05:16,954 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2025-04-16 22:05:17,960 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2025-04-16 22:05:21,983 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2025-04-16 22:05:22,988 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-16 22:05:23,993 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2025-04-16 22:05:26,007 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-04-16 22:05:28,018 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "2025-04-16 22:05:30,031 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-04-16 22:05:31,036 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2025-04-16 22:05:34,052 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2025-04-16 22:05:37,072 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-04-16 22:05:38,077 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2025-04-16 22:05:39,082 INFO mapreduce.Job:  map 53% reduce 6%\n",
      "2025-04-16 22:05:40,087 INFO mapreduce.Job:  map 57% reduce 6%\n",
      "2025-04-16 22:05:42,098 INFO mapreduce.Job:  map 60% reduce 6%\n",
      "2025-04-16 22:05:43,107 INFO mapreduce.Job:  map 60% reduce 13%\n",
      "2025-04-16 22:05:44,113 INFO mapreduce.Job:  map 63% reduce 13%\n",
      "2025-04-16 22:05:45,119 INFO mapreduce.Job:  map 63% reduce 14%\n",
      "2025-04-16 22:05:46,124 INFO mapreduce.Job:  map 67% reduce 14%\n",
      "2025-04-16 22:05:47,129 INFO mapreduce.Job:  map 67% reduce 21%\n",
      "2025-04-16 22:05:48,134 INFO mapreduce.Job:  map 70% reduce 21%\n",
      "2025-04-16 22:05:49,139 INFO mapreduce.Job:  map 70% reduce 22%\n",
      "2025-04-16 22:05:50,144 INFO mapreduce.Job:  map 73% reduce 22%\n",
      "2025-04-16 22:05:51,150 INFO mapreduce.Job:  map 73% reduce 23%\n",
      "2025-04-16 22:05:52,156 INFO mapreduce.Job:  map 77% reduce 23%\n",
      "2025-04-16 22:05:53,161 INFO mapreduce.Job:  map 77% reduce 24%\n",
      "2025-04-16 22:05:54,165 INFO mapreduce.Job:  map 80% reduce 24%\n",
      "2025-04-16 22:05:55,170 INFO mapreduce.Job:  map 80% reduce 26%\n",
      "2025-04-16 22:05:56,175 INFO mapreduce.Job:  map 83% reduce 26%\n",
      "2025-04-16 22:05:57,180 INFO mapreduce.Job:  map 83% reduce 27%\n",
      "2025-04-16 22:05:58,185 INFO mapreduce.Job:  map 87% reduce 27%\n",
      "2025-04-16 22:05:59,190 INFO mapreduce.Job:  map 87% reduce 28%\n",
      "2025-04-16 22:06:00,196 INFO mapreduce.Job:  map 90% reduce 28%\n",
      "2025-04-16 22:06:01,206 INFO mapreduce.Job:  map 90% reduce 29%\n",
      "2025-04-16 22:06:02,210 INFO mapreduce.Job:  map 97% reduce 29%\n",
      "2025-04-16 22:06:03,216 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2025-04-16 22:06:04,221 INFO mapreduce.Job:  map 100% reduce 54%\n",
      "2025-04-16 22:06:06,231 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "2025-04-16 22:06:07,236 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-16 22:06:07,244 INFO mapreduce.Job: Job job_1744823371038_0043 completed successfully\n",
      "2025-04-16 22:06:07,318 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1949909\n",
      "\t\tFILE: Number of bytes written=13888677\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=39222272\n",
      "\t\tHDFS: Number of bytes written=1530\n",
      "\t\tHDFS: Number of read operations=105\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=13\n",
      "\t\tRack-local map tasks=17\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=259821\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=332517\n",
      "\t\tTotal time spent by all map tasks (ms)=86607\n",
      "\t\tTotal time spent by all reduce tasks (ms)=110839\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=86607\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=110839\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=266056704\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=340497408\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=700000\n",
      "\t\tMap output records=700000\n",
      "\t\tMap output bytes=26643383\n",
      "\t\tMap output materialized bytes=3899498\n",
      "\t\tInput split bytes=4170\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=274260\n",
      "\t\tReduce shuffle bytes=3899498\n",
      "\t\tReduce input records=700000\n",
      "\t\tReduce output records=40\n",
      "\t\tSpilled Records=1400000\n",
      "\t\tShuffled Maps =90\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=90\n",
      "\t\tGC time elapsed (ms)=2082\n",
      "\t\tCPU time spent (ms)=50210\n",
      "\t\tPhysical memory (bytes) snapshot=10387271680\n",
      "\t\tVirtual memory (bytes) snapshot=143085047808\n",
      "\t\tTotal committed heap usage (bytes)=9570353152\n",
      "\t\tPeak Map Physical memory (bytes)=373809152\n",
      "\t\tPeak Map Virtual memory (bytes)=4342886400\n",
      "\t\tPeak Reduce Physical memory (bytes)=261349376\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4349530112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39218102\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1530\n",
      "2025-04-16 22:06:07,318 INFO streaming.StreamJob: Output directory: /user/websites\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/websites\n",
    "!yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D stream.map.output.field.separator=+\\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D map.output.key.field.separator=+ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapreduce.job.name='websites' \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files mp_for_website.py,reduce_for_website.py \\\n",
    "-mapper \"python3 mp_for_website.py map\"\\\n",
    "-reducer \"python3 reduce_for_website.py reducer\"\\\n",
    "-input /user/Посещения.csv \\\n",
    "-output /user/websites\\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c923ab9",
   "metadata": {},
   "source": [
    "Устанавливаем key.partitioner=k1,1, чтобы ключи разделялись по датам "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebfa8ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 https://gonzales-bautista.com/ 353\r\n",
      "2024-05-30 https://smith.com/ 246\r\n",
      "2024-05-30 https://www.smith.com/ 239\r\n",
      "2024-05-30 http://smith.com/ 229\r\n",
      "2024-05-30 http://www.smith.com/ 225\r\n",
      "2024-06-02 http://smith.com/ 7\r\n",
      "2024-06-02 https://gonzales-bautista.com/ 7\r\n",
      "2024-06-02 https://www.williams.com/ 6\r\n",
      "2024-06-02 http://lee.com/ 5\r\n",
      "2024-06-02 http://miller.com/ 5\r\n",
      "2024-05-28 https://gonzales-bautista.com/ 368\r\n",
      "2024-05-28 https://smith.com/ 256\r\n",
      "2024-05-28 https://www.smith.com/ 251\r\n",
      "2024-05-28 http://smith.com/ 224\r\n",
      "2024-05-28 http://www.smith.com/ 204\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/websites/* | grep -E '2024-05-28|2024-06-02|2024-05-30' | column -t -s$'\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761992e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
